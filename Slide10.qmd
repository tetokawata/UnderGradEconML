---
title: "前処理"
subtitle: "テキスト分析"
author: "川田恵介"
format: 
  revealjs:
    html-math-method: katex
    slide-number: true
self-contained: true
self-contained-math: true
execute: 
  warning: false
  message: false
  eval: true
  echo: false
---

```{r}
#| label: SetUp

pacman::p_load(
  tidyverse,
  quanteda,
  quanteda.textstats,
  SuperLearner,
  wordcloud2,
  htmlwidgets
  )

Data <- read_csv("Public/Text.csv")

Tokes <- Data |> 
  corpus(
  text_field = "Title") |> 
  tokens() |>
  tokens_remove(
    c(
      stopwords::stopwords("ja", source = "marimo"),
      stopwords::stopwords("ja", source = "stopwords-iso")
      )
  )

Seqs <- Tokes |> 
    textstat_collocations(min_count = 2, tolower = FALSE)

Seqs <- Seqs[Seqs$z > 5,]

TokesSeq <- Tokes |> 
  tokens_select(min_nchar = 2) |> 
  tokens_compound(Seqs, concatenator = '')
```

# 前処理: 発展

- tokens化しただけでは、一般に不十分

    - 新語への対応
    
    - 大量のあまり意味のない単語(助詞や助動詞)が含まれる

- 文章の特徴把握や予測モデル構築を困難にする

- 対応策

    - 辞書の更新
    
    - 統計的基準の導入

## 新語への対応

- 常に新しい単語が出現する

    - 辞書だけに頼った前処理では、対応が不十分になってしまう

    - 分かち書きをしていない日本語では特に深刻
    
- 例: 新型コロナ感染症 $\rightarrow$ 新型/ コロナ/ 感染症

## 共起語の接続

- 文章中に連続して出現しやすい単語

    - 一つの単語として接続する

    - 統計的に判定可能


```{r}
Seqs
```

## Stop wordsの除去

- 文章に大量に出現する、「意味のない」、単語

    - 一般に除去することが望ましい

- 方法

    - 辞書の活用

    - 統計的基準を導入 (1文字言葉の除去など)

## Rare wordsの除去

- 文章に一度しか出てこない単語は、予測に活用不可能

    - 除去すべき

- 非常にまれ/ほとんど出てくる単語についても?


## 例: WordCloud

```{r}
#| label: WordCloud

Temp <- Tokes |> 
  dfm() |> 
  dfm_trim(
    min_termfreq = 2
  )

topfeatures(Temp,20)
```

## 例: WordCloud

```{r}
#| label: WordCloudClean

Temp <- TokesSeq |> 
  dfm() |> 
  dfm_trim(
    min_termfreq = 2
  )

topfeatures(Temp,20)
```

## まとめ

- テキスト変数の前処理は、まだまだ研究が続いている

    - 例えば表記ブレをどのように対処するか?
    
- 推定方法の面での対応も必須