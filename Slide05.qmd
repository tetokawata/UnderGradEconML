---
title: "決定木アルゴリズム: モデル集計"
subtitle: "経済学のための機械学習入門"
author: "川田恵介"
format: 
  revealjs:
    html-math-method: katex
    slide-number: true
    incremental: true
self-contained: true
self-contained-math: true
execute: 
  warning: false
  message: false
  eval: true
  echo: false
---

# モデル集計

```{r}
pacman::p_load(
  tidyverse,
  SuperLearner,
  rpart,
  ranger
)

Data <- read_csv(
  "Public/Example.csv"
  ) |> 
  select(-District)

Noise <- 50
FigLower <- -20
FigUpper <- 60

N <- 100

SimData <- function(i,n){
  set.seed(i)
  TempData <- tibble(
    X = runif(n,-4,4),
    TrueY = 0.5*(X^2) + 
      if_else(X <= -2,
              10,
              0) + 
      if_else(X <= 0,
              10,
              0) + 
      if_else(X <= 2,
              10,
              0),
    Y = TrueY + runif(n,-Noise,Noise)
  )
  return(TempData)
}

TestData <- SimData(100,10000)

EstDeepTree <- function(i,n){
  TempData <- SimData(i,n)
  TempPred <- rpart::rpart(
    Y ~ X,
    TempData,
    control = rpart::rpart.control(
      cp = 0,
      maxdepth = 30,
      minsplit = 50,
      minbucket = 50
    )
  ) |> 
    predict(TestData)
  TempResult <- TestData |> 
    mutate(
    Pred = TempPred,
    ID = i |> factor()
  )
  return(TempResult)
}

TempResult <- EstDeepTree(1,1000) |> 
  bind_rows(
    EstDeepTree(2,1000)
  ) |> 
  bind_rows(
    EstDeepTree(3,1000)
  ) |> 
  bind_rows(
    EstDeepTree(4,1000)
  ) |> 
  mutate(
    Aggregate = mean(Pred),
    .by = "X")
```

- データ分析の基本的発想: 事例を集計することで、観察できない要因の偏りの影響を緩和

    - 予測モデルへの影響はどうしても残る
    
- 新しい発想: 予測モデル自体を"集計"する

## 比喩: 予測屋会議

- 複数の"専門家"の予測を集計して最終予測モデルとする

    - "エコノミスト"の見通しの平均値
    
    - 専門家委員会

- 一人の予測に頼るよりも、ましでは？

    - 教師付き学習にも応用可能な発想

## 数値例

- 独立して収集したデータについて、深い予測木 (剪定なし) を推定

- 各予測値と、予測値の平均を比較

## 予測結果

```{r, dev='ragg_png'}
TempResult |> 
  ggplot(
    aes(
      x = X,
      y = Pred,
      color = ID
    )
  ) +
  theme_bw() +
  geom_point() +
  geom_smooth(
    aes(
      y = TrueY,
      color = "Population"
    ),
    method = "lm",
    formula = y ~ 0 + I(x^2) + I(if_else(x <= -2,1,0))
    + I(if_else(x <= 0,1,0))
    + I(if_else(x <= 2,1,0))
    ) +
  facet_wrap(
    ~ ID
  ) +
  ylim(FigLower,FigUpper) +
  ylab("E[Y|X]")
```

## 集計

```{r, dev='ragg_png'}
TempResult |> 
  ggplot(
    aes(
      x = X,
      y = Pred,
      color = ID
    )
  ) +
  theme_bw() +
  geom_point(
    size = 0.3
    ) +
  geom_point(
    aes(
      y = Aggregate,
      color = "平均"
    )
  ) +
  geom_smooth(
    aes(
      y = TrueY,
      color = "Population"
    ),
    method = "lm",
    formula = y ~ 0 + I(x^2) + I(if_else(x <= -2,1,0))
    + I(if_else(x <= 0,1,0))
    + I(if_else(x <= 2,1,0))
  ) +
  ylim(FigLower,FigUpper) +
  ylab("E[Y|X]")
```

## チャレンジ

- 「独立して抽出された」有限個データから生成された予測モデル

- 「独立して抽出した複数のデータから得た」予測モデルの集計は通常不可能

    - 推定に使ったサンプルサイズが実質的に増えているので、性能改善は"当たり前"

- 近似的に行う

    - (Nonparametric) bootstrapの活用

# Bootstrap Aggregating

- Bagging

## 決定木の不安定性

- 現実は複雑なので、複雑なモデル(巨大な木)が本来は望ましい

    - 事例数が限られている場合、データ固有の特徴を強く反映してしまう
    
    - 適度に単純化する必要があるが、、、、

- 多くの実践で、決定木推定の不安定性( $=$ データ固有の特徴を強く反映してしまう)は、剪定を行っても十分に緩和できない

    - Bootstrapでデータを複製して、モデル集計

## 理想のBagging

```{mermaid}
flowchart TB
  A[Prediction Task] --> B{Random Sampling}
  B --> C[Data 1]
  B --> D[Data 2]
  B --> E[Data 3]
  C --> F[Tree 1]
  D --> G[Tree 2]
  E --> H[Tree 3]
  F --> J[Aggregate]
  G --> J
  H --> J
```

## アルゴリズム

1. Nonparametric bootstrapで、データの複製を行う (500,1000,2000など)

2. 各複製データについて、"深い"決定木を推定

3. 各 $X$ についての予測値の平均を最終予測値とする


## 補論: Bootstrap


```{mermaid}
flowchart TB
  B{Original Data ID 1,2,3} --> C[ID 1,2,2]
  B --> D[ID 2,3,1]
  B --> E[ID 3,3,2]
  C --> F[Tree 1]
  D --> G[Tree 2]
  E --> H[Tree 3]
  F --> J[Aggregate]
  G --> J
  H --> J
```


## Baggingの発想

- 基本アイディア: 非常に深い木を生成すれば、予測結果が不安定になるが、

- 平均を取れば、安定する

    - 独立・無相関であれば、 無限個の**複製データ**から予測モデルを作れば、分散を0にできる
    
    - 今のPCであれば、大量の予測モデルの生成は可能

## Baggingの限界
    
- リーマンショック、地震保険が(火災保険などと比べて)難しい理由は？

    - 事象間での相関
    
- よく似た予測結果ばかりであれば、平均をとってもあまり意味がない

    - 予測結果を十分に"散らばらせる"必要がある
    
    - 金融ポートフォリオであれば、平均的なリターンが低く買ったとしても、”海外”の商品も組み込むなど

## RandomForest

- データ分割に用いることができる変数群をランダムに選ぶ

- 例：ある予測木の第n分割を行う際に

    - Bagging: $\{$ 年齢、性別、学歴 $\}$ から選ぶ
    
    - Random Forest: $\{$ 年齢、性別 $\}$ から選ぶ
    
        - 第 $n+1$ 分割を行う際には、 $\{$ 学歴、性別 $\}$

## RandomForest: 動機

- 動機: 予測値同士の相関を弱める

    - 相関を強める要因(データが多少変わっても、同じような変数を活用する)を排除

    - そこそこの予測力を持つ変数が、強力な予測力を持つ変数の陰に隠れてしまうことを避けられる

## 数値例

```{r}
Noise <- 10
SimData <- function(i,n){
  set.seed(i)
  TempData <- tibble(
    X = runif(n,-4,4),
    D = if_else(X >= 0, 
                sample(0:1,n,replace = TRUE, prob = c(95/100,5/100)),
                sample(0:1,n,replace = TRUE, prob = c(5/100,95/100))
                ),
    TrueY = X + 0.5*D,
    Y = TrueY + runif(n,-Noise,Noise)
  ) |> 
    mutate(D = factor(D))
  return(TempData)
}

TestData <- SimData(100,10000)

TestData |> 
  ggplot(
    aes(
      x = X,
      y = TrueY,
      color = D
    )
  ) +
  theme_bw() +
  geom_line()
```

## 数値例

```{r}
Fit <- ranger::ranger(
  Y ~ D + X,
  SimData(1,3000),
  mtry = 2,
  max.depth = 3
  )

Pred <- predict(
  Fit,
  TestData,
  predict.all = TRUE
)

TestData |> 
  mutate(
    Pred = Pred$predictions[,1],
    ID = 1
  ) |> 
  bind_rows(
    TestData |> 
      mutate(
        Pred = Pred$predictions[,2],
        ID = 2
        )
    ) |> 
  bind_rows(
    TestData |> 
      mutate(
        Pred = Pred$predictions[,3],
        ID = 3
        )
    ) |> 
  bind_rows(
    TestData |> 
      mutate(
        Pred = Pred$predictions[,4],
        ID = 4
        )
    ) |> 
  ggplot(
    aes(
      x = X,
      y = Pred,
      color = D
    )
  ) +
  theme_bw() +
  geom_line(
    aes(
      y = TrueY,
      color = D
    )
  ) +
  geom_line() +
  facet_wrap(
    ~ ID
  ) +
  ylab("E[Y|X]")
```

## 数値例

```{r}
Fit <- ranger::ranger(
  Y ~ D + X,
  SimData(1,3000),
  mtry = 1,
  max.depth = 3
  )

Pred <- predict(
  Fit,
  TestData,
  predict.all = TRUE
)

TestData |> 
  mutate(
    Pred = Pred$predictions[,1],
    ID = 1
  ) |> 
  bind_rows(
    TestData |> 
      mutate(
        Pred = Pred$predictions[,2],
        ID = 2
        )
    ) |> 
  bind_rows(
    TestData |> 
      mutate(
        Pred = Pred$predictions[,3],
        ID = 3
        )
    ) |> 
  bind_rows(
    TestData |> 
      mutate(
        Pred = Pred$predictions[,4],
        ID = 4
        )
    ) |> 
  ggplot(
    aes(
      x = X,
      y = Pred,
      color = D
    )
  ) +
  theme_bw() +
  geom_line(
    aes(
      y = TrueY,
      color = D
    )
  ) +
  geom_line() +
  facet_wrap(
    ~ ID
  ) +
  ylab("E[Y|X]")
```

## 実例

```{r}
#| echo: true
X <- select(Data, -Price)

Y <- Data$Price

Fit <- SuperLearner(
  X = X,
  Y = Y,
  SL.library = c(
    "SL.mean",
    "SL.rpart",
    "SL.rpartPrune",
    "SL.ranger"
  )
  )

Fit$cvRisk
```

# まとめ

- Resampling $=$ 元々のデータから、"新しい"データを作り出す

- Resamplingは現代のデータ分析において、強力な手法

    - 交差検証(事例の被りは許さない): モデル評価など
    
    - Bootstrap: モデルの改善(Bagging/RandomForest) 
    
    - 伝統的なInferenceへのBootstrapの応用も、もちろん重要
